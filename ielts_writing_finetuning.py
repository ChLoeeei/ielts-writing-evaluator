# -*- coding: utf-8 -*-
"""Ielts-writing-finetuning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Kf3vUXNpZUIfYEF20IJ9wruhyMckR1rl
"""

!pip install unsloth==2025.3.18 unsloth-zoo==2025.3.16 datasets

!pip install transformers wandb

!pip install unsloth

# IELTS Writing Evaluator Fine-tuning with Llama 3.2-3B
# Optimized for Google Colab and Ollama deployment
import torch
import pandas as pd
import numpy as np
from datasets import Dataset
from sklearn.model_selection import train_test_split
from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling
from unsloth import FastLanguageModel, is_bfloat16_supported
import wandb
from huggingface_hub import login

# --- Configuration ---
CSV_PATH = "ielts_writing_dataset.csv"
MODEL_NAME = "unsloth/Llama-3.2-3B-Instruct-bnb-4bit"
OUTPUT_DIR = "./ielts_writing_evaluator"
HF_REPO_NAME = "chloemeow/ielts-writing-evaluator"
GITHUB_REPO = "ChLoeeei/ielts-writing-evaluator"

# Model parameters - optimized for Colab
MAX_SEQ_LENGTH = 2048
LOAD_IN_4BIT = True
DTYPE = None  # Let unsloth decide automatically

# Credentials
HF_TOKEN = ""
wandb_key = ""

print("ğŸš€ Starting IELTS Writing Evaluator Fine-tuning")
print(f"ğŸ“Š Loading dataset from: {CSV_PATH}")

# --- Dataset Loading and Preparation ---
try:
    dataset_df = pd.read_csv(CSV_PATH)
    # Clean the dataset
    dataset_df = dataset_df[['Question', 'Essay', 'Overall']].dropna().reset_index(drop=True)
    print(f"âœ… Loaded {len(dataset_df)} essays with scores.")
    print(f"ğŸ“ˆ Score distribution:")
    print(dataset_df['Overall'].value_counts().sort_index())
except Exception as e:
    print(f"âŒ Error reading CSV: {e}")
    exit()

# --- Enhanced Dataset Formatting ---
def format_instruction(sample):
    """
    Enhanced format for both evaluation and teaching capabilities
    """
    question = sample['Question']
    essay = sample['Essay']
    overall_score = sample['Overall']

    # Create more detailed evaluation template
    instruction = f"""<|begin_of_text|><|start_header_id|>system<|end_header_id|>

You are an expert IELTS writing examiner and teacher. Your task is to evaluate essays and provide educational feedback to help students improve their writing skills.

Evaluation Criteria:
1. Task Achievement (TA): How well the essay addresses the task
2. Coherence and Cohesion (CC): Organization and logical flow
3. Lexical Resource (LR): Vocabulary range and accuracy
4. Grammatical Range and Accuracy (GRA): Grammar complexity and correctness

Each criterion is scored from 1-9, and the overall band score is calculated accordingly.<|eot_id|><|start_header_id|>user<|end_header_id|>

Please evaluate this IELTS Writing Task 2 essay:

**Question:** {question}

**Student Essay:** {essay}

Please provide:
1. Overall band score and individual criterion scores
2. Detailed analysis for each criterion
3. Specific examples from the essay
4. Improvement suggestions
5. Sample corrections for key errors<|eot_id|><|start_header_id|>assistant<|end_header_id|>

## IELTS Writing Evaluation

### Overall Band Score: {overall_score}/9.0

### Detailed Analysis:

**Task Achievement:** The essay {"addresses the task effectively" if float(overall_score) >= 7 else "partially addresses the task requirements" if float(overall_score) >= 5 else "has limited task response"}.

**Coherence and Cohesion:** The essay {"demonstrates clear organization with effective paragraphing" if float(overall_score) >= 7 else "shows some organization but may lack clear progression" if float(overall_score) >= 5 else "has limited coherence and cohesion"}.

**Lexical Resource:** The vocabulary {"shows good range and flexibility" if float(overall_score) >= 7 else "is adequate but may be repetitive" if float(overall_score) >= 5 else "is limited with frequent errors"}.

**Grammatical Range and Accuracy:** The grammar {"demonstrates variety with good control" if float(overall_score) >= 7 else "shows some variety but with errors" if float(overall_score) >= 5 else "is limited with frequent errors"}.

### Key Improvement Areas:
- {"Focus on developing more complex arguments" if float(overall_score) < 7 else "Continue to refine argument sophistication"}
- {"Work on paragraph transitions and logical flow" if float(overall_score) < 6 else "Maintain good organizational structure"}
- {"Expand vocabulary range and accuracy" if float(overall_score) < 6 else "Continue building advanced vocabulary"}
- {"Practice complex sentence structures" if float(overall_score) < 6 else "Maintain grammatical accuracy"}

### Study Recommendations:
1. **Reading:** Study high-scoring IELTS essays in similar topics
2. **Practice:** Write essays with timed conditions (40 minutes)
3. **Vocabulary:** Build topic-specific vocabulary banks
4. **Grammar:** Focus on complex sentence structures and accuracy<|eot_id|>"""

    return {"text": instruction}

# Create datasets
print("ğŸ“ Formatting dataset...")
formatted_data = [format_instruction(row) for _, row in dataset_df.iterrows()]
dataset = Dataset.from_list(formatted_data)

# Split dataset - handle classes with only 1 sample
print("ğŸ“Š Handling dataset split...")

# Check class distribution
score_counts = dataset_df['Overall'].value_counts()
print(f"Score distribution: {dict(score_counts)}")

# Find classes with only 1 sample
single_sample_classes = score_counts[score_counts == 1].index.tolist()
if single_sample_classes:
    print(f"âš ï¸  Classes with only 1 sample: {single_sample_classes}")
    print("Using random split instead of stratified split to handle rare classes.")

    # Use random split for better handling of rare classes
    train_dataset, eval_dataset = train_test_split(
        formatted_data,
        test_size=0.15,  # Slightly larger eval set to ensure diversity
        random_state=42,
        shuffle=True
    )
else:
    # Use stratified split if all classes have multiple samples
    train_dataset, eval_dataset = train_test_split(
        formatted_data,
        test_size=0.1,
        random_state=42,
        stratify=dataset_df['Overall']
    )

train_dataset = Dataset.from_list(train_dataset)
eval_dataset = Dataset.from_list(eval_dataset)

print(f"ğŸ“š Training set: {len(train_dataset)} samples")
print(f"ğŸ“Š Evaluation set: {len(eval_dataset)} samples")

# --- Authentication ---
print("Authenticating...")
login(token=HF_TOKEN)
wandb.login(key=wandb_key)

wandb.init(
    project="ielts_writing_evaluator",
    name="llama3.2-3b-ielts-v1",
    config={
        "model": MODEL_NAME,
        "max_seq_length": MAX_SEQ_LENGTH,
        "dataset_size": len(dataset_df)
    }
)

!pip uninstall unsloth
!pip install --upgrade unsloth

# --- Model Loading ---
print(f"ğŸ¤– Loading model: {MODEL_NAME}")
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name=MODEL_NAME,
    max_seq_length=MAX_SEQ_LENGTH,
    dtype=DTYPE,
    load_in_4bit=LOAD_IN_4BIT,
    trust_remote_code=True,
)

# Configure LoRA
print("âš¡ Setting up LoRA configuration...")
model = FastLanguageModel.get_peft_model(
    model,
    r=32,  # Increased rank for better performance
    target_modules=[
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj",
    ],
    lora_alpha=32,
    lora_dropout=0.05,
    bias="none",
    use_gradient_checkpointing="unsloth",
    random_state=3407,
    use_rslora=False,
    loftq_config=None,
)

# --- Training Configuration ---
training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    num_train_epochs=3,
    per_device_train_batch_size=1,  # Reduced for Colab
    per_device_eval_batch_size=1,
    gradient_accumulation_steps=8,  # Increased to maintain effective batch size
    eval_strategy="steps",
    eval_steps=50,
    save_strategy="steps",
    save_steps=100,
    save_total_limit=2,
    load_best_model_at_end=True,
    logging_steps=10,
    learning_rate=2e-4,
    warmup_steps=50,
    optim="adamw_8bit",
    weight_decay=0.01,
    remove_unused_columns=False,
    report_to="wandb",
    dataloader_pin_memory=False,  # Reduce memory usage
    fp16=not is_bfloat16_supported(),
    bf16=is_bfloat16_supported(),
    group_by_length=True,  # Efficient batching
    ddp_find_unused_parameters=False,
)

# --- Tokenization ---
def tokenize_function(examples):
    """Tokenize the formatted instructions"""
    tokenized = tokenizer(
        examples["text"],
        padding=False,  # Dynamic padding is more memory efficient
        truncation=True,
        max_length=MAX_SEQ_LENGTH,
        return_tensors=None,
    )
    tokenized["labels"] = tokenized["input_ids"].copy()
    return tokenized

print("ğŸ”¤ Tokenizing datasets...")
tokenized_train_dataset = train_dataset.map(
    tokenize_function,
    batched=True,
    remove_columns=["text"],
    desc="Tokenizing train dataset"
)

tokenized_eval_dataset = eval_dataset.map(
    tokenize_function,
    batched=True,
    remove_columns=["text"],
    desc="Tokenizing eval dataset"
)

# Data collator for dynamic padding
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False,
    pad_to_multiple_of=8,  # Optimize for tensor cores
)

# --- Training ---
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train_dataset,
    eval_dataset=tokenized_eval_dataset,
    data_collator=data_collator,
)

print("Starting training...")
trainer.train()

# --- Model Saving ---
print("ğŸ’¾ Saving model...")
model.save_pretrained(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)

print("ğŸ“¤ Saving to Hugging Face Hub...")
model.push_to_hub(HF_REPO_NAME, token=HF_TOKEN)
tokenizer.push_to_hub(HF_REPO_NAME, token=HF_TOKEN)

# --- Prepare for Ollama ---
print("ğŸ”„ Converting for Ollama deployment...")

# Save in format suitable for Ollama
ollama_dir = "./ollama_model"
model.save_pretrained_merged(ollama_dir, tokenizer, save_method="merged_16bit")

print(f"""
âœ… Training Complete!

ğŸ“ Model saved to: {OUTPUT_DIR}
ğŸ¤— Hugging Face: {HF_REPO_NAME}
ğŸ¦™ Ollama model: {ollama_dir}

ğŸš€ To deploy with Ollama:
1. Copy the model files to your Ollama models directory
2. Create a Modelfile with your model configuration
3. Run: ollama create ielts-evaluator -f Modelfile

ğŸ“ Example Modelfile:
FROM {ollama_dir}
TEMPLATE \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>
You are an expert IELTS writing examiner and teacher.
<|eot_id|><|start_header_id|>user<|end_header_id|>
{{ .Prompt }}<|eot_id|><|start_header_id|>assistant<|end_header_id|>
\"\"\"
PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER stop <|eot_id|>
""")

# Clean up
wandb.finish()
print("ğŸ‰ All done! Your IELTS Writing Evaluator is ready!")

model.config.to_json_file("config.json")

!zip -r ollama_model.zip ollama_model



from google.colab import drive
drive.mount('/content/drive')


!cp /content/ollama_model.zip /content/drive/MyDrive/

# Load model directly
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("chloemeow/ielts-writing-evaluator")
model = AutoModelForCausalLM.from_pretrained("chloemeow/ielts-writing-evaluator")

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# 3. å®šä¹‰è¦è¯„åˆ†çš„é›…æ€ä½œæ–‡ï¼ˆè‹±æ–‡åŸæ–‡ï¼‰
essay_text = """
Opinions diverge among young adults pursuing tertiary education currently. While some of them opt for a multidisciplinary route, others concentrate on one single major. There are valid reasons for both sides, which will be further explained in this essay before presenting my own opinion.

For proponents of multi-course strategy, the reasons involve the recognition that the job market in the existing society requires well-rounded talents rather than professional experts. This holds some truth for general majors such as English/Chinese Studies, Marketing and Business. To have a competitive edge over other graduates and would-be editors, sellers, and managers, being equipped with additional knowledge of related fields can be a viable approach. Coveting having a minor apart from the courses in their own major can be even more desirable for numerous students unsure of their future jobs. Probably by having a taste of distinct courses, they can find a major that ignites their inner pursuit and fully utilise their aptitudes for a proper future career.

However, there are certain cases where allocating time to various courses in different majors is not a rule of thumb. This particularly applies to those majors requiring arduous effort and resilience. One prime example is would-be lawyers and doctors. To successfully earn qualification, these graduates should be devoted to their key courses, learning an excessive number of past court cases and symptoms by heart before they can garner a decent and satisfactory job. Were it not for their years of endeavour in their studies, they would not be qualified workers in relevant fields.

In conclusion, whether choosing to delve into one major or striving for several various subjects is contingent on the nature of majors. While those majors entailing a significant amount of professional expertise give no spare time for undergraduates, college students with no clear vision of their future jobs and more ambitious ones can assume more control over their choice of majors.
"""

# 4. æ„å»ºæ”¹è¿›åçš„ promptï¼Œæ˜ç¡®è¦æ±‚ä¸­æ–‡å›ç­”å¹¶ç»™å‡º 1â€“9 åˆ†æ•°
prompt = f"""è¯·ç”¨ä¸­æ–‡å¯¹ä¸‹é¢çš„ IELTS Task 2 ä½œæ–‡è¿›è¡Œè¯„ä»·ï¼Œå¹¶ç»™å‡ºä» 1 åˆ° 9 çš„â€œæ•´ä½“åˆ†æ•°â€ï¼Œæœ€åç”¨å‡ å¥è¯è¯´æ˜è¯„åˆ†ç†ç”±ã€‚

ã€ä½œæ–‡åŸæ–‡ã€‘
{essay_text}

ã€è¯„ä»·ã€‘
"""

# 5. å¯¹ prompt è¿›è¡Œç¼–ç ï¼Œå¹¶å°†å¼ é‡ç§»åŠ¨åˆ° GPU
inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1024)
inputs = {k: v.to(device) for k, v in inputs.items()}

# 6. ç”¨æ¨¡å‹ç”Ÿæˆç­”æ¡ˆ
with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_new_tokens=200,       # æœ€å¤šç”Ÿæˆ 200 ä¸ªæ–° token
        do_sample=False,          # å…³é—­é‡‡æ ·ä»¥è·å¾—æ›´ç¨³å®šçš„è¾“å‡º
        num_beams=3               # ä½¿ç”¨ beam search æ¥æé«˜ç­”æ¡ˆè´¨é‡
    )

# 7. è§£ç è¾“å‡ºç»“æœå¹¶æ‰“å°
result = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(result)